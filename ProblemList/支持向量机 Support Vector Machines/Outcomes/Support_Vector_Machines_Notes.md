# Support Vector Machines Notes

https://blog.csdn.net/weixin_44378835/article/details/110732412

https://blog.csdn.net/liweibin1994/article/details/77504210

## 定义

对于线性可分的两类问题，其分类决策边界为一 $n$ 维特征空间中的超平面 $H$，一般情况下会有无穷多个解。当我们确定了一个解对应的权向量 $\omega$，超平面的斜率和朝向就是确定的了，可以在一定的范围内平移超平面 $H$，只要不达到或者越过两类中距离 $H$ 最近的样本，分类决策边界都可以正确地实现线性分类。所以，任何一个求解得到的权向量 $\omega$，都会带来一系列平行的分类决策边界，其可平移的范围具有一定的宽度，称为分类间隔 $d$ (Margin of Classification)。

![img](https://img-blog.csdnimg.cn/20201206122540665.png)

显然，当我们改变 $\omega$，使分类决策边界的斜率和朝向随之变化时，我们得到的分类间隔是不同的。那么，分类间隔究竟是大还是小更好呢？当然是越大越好。因为分类间隔越大，两类样本做分类决策时的裕量也就越大，由于样本采集所带来的特征值误差所造成的分类错误也就越少。所以，在所有能够实现正确分类的权向量 $\omega$ 中，去求取到分类间隔最大的一个 $\omega^*$ ，就是对线性分类器进行优化求解的一个很好的指标。这就是“支持向量机”这种线性分类器训练算法的出发点。

## 需要用到的知识

###    高数中的拉格朗日乘子（已学）

******************

<font size=5 color=red>**这段实在是太复杂了，建议直接跳过直接看结论!!!**</font>

### 矩阵分析里的拉格朗日乘子

* ####    包含多个等式约束的最优化问题

函数$f(x)=x^T\omega+b$存在约束条件$g_k(x)=x^T\omega_k+b_k=0,\space k=1,2\cdots ,N$

则求最小值问题可构建拉格朗日函数

$$L\left(x,\lambda\right)=f(x)+\sum_{k=1}^{N}\lambda_kg_k(x)$$

求偏导后联立方程

$$\frac{\partial f(x)}{\partial x*}+\sum_{k=1}^N\frac{\partial}{\partial x*}\lambda_kg_k(x)=0$$

* #### 含有不等式约束（拉格朗日对偶问题）

两条性质：

1. $f(x)=\max\{x_1,x_2\cdots,x_n\}$在$\R^n$上是凸函数。
2. $f(x)=\min\{x_1,x_2\cdots,x_n\}$在$\R^n$上是凹函数。

证明略

**$$\vdots$$**

总之就是用一堆定理推导出了一个通式！不要在上面浪费时间！

******************

## 结论
