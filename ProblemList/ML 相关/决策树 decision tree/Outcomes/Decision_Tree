# Decision Tree



## 适用范围

* 如果输入数据联系非常紧密，且有优先关系
  * 这样就满足一个树形结构，就比较容易通过决策树学习得出
    * 比如人脸，往往是先由几个关键部分决定的
    
  * ## 材料

https://blog.csdn.net/u012328159/article/details/70184415

https://blog.csdn.net/u012328159/article/details/79285214

决策树是一种机器学习的方法。决策树的生成算法有ID3, C4.5和C5.0等。决策树是一种树形结构，其中每个内部节点表示一个属性上的判断，每个分支代表一个判断结果的输出，最后每个叶节点代表一种分类结果。

决策树是一种十分常用的分类方法，需要监管学习（有教师的Supervised Learning），监管学习就是给出一堆样本，每个样本都有一组属性和一个分类结果，也就是分类结果已知，那么通过学习这些样本得到一个决策树，这个决策树能够对新的数据给出正确的分类。这里通过一个简单的例子来说明决策树的构成思路：

给出如下的一组数据，一共有十个样本（学生数量），每个样本有分数，出勤率，回答问题次数，作业提交率四个属性，最后判断这些学生是否是好学生。最后一列给出了人工分类结果。

![img](https://pic3.zhimg.com/80/v2-ed38beb4538a90f2b961233b18acc1ca_720w.jpg)

然后用这一组附带分类结果的样本可以训练出多种多样的决策树，这里为了简化过程，我们假设决策树为二叉树，且类似于下图：



![img](https://pic1.zhimg.com/80/v2-ff4fe0d16ec17c5520837b3aad52ed54_720w.jpg)



通过学习上表的数据，可以设置A，B，C，D，E的具体值，而A，B，C，D，E则称为阈值。当然也可以有和上图完全不同的树形，比如下图这种的：

![img](https://pic3.zhimg.com/80/v2-8f6407e5ab5a58b2913aef6a332090f6_720w.jpg)

> [ID3](https://en.wikipedia.org/wiki/ID3_algorithm) (Iterative Dichotomiser 3) was developed in 1986 by Ross Quinlan. The algorithm creates a multiway tree, finding for each node (i.e. in a greedy manner) the categorical feature that will yield the largest information gain for categorical targets. Trees are grown to their maximum size and then a pruning step is usually applied to improve the ability of the tree to generalise to unseen data.

> C4.5 is the successor to ID3 and removed the restriction that features must be categorical by dynamically defining a discrete attribute (based on numerical variables) that partitions the continuous attribute value into a discrete set of intervals. C4.5 converts the trained trees (i.e. the output of the ID3 algorithm) into sets of if-then rules. These accuracy of each rule is then evaluated to determine the order in which they should be applied. Pruning is done by removing a rule’s precondition if the accuracy of the rule improves without it.

> C5.0 is Quinlan’s latest version release under a proprietary license. It uses less memory and builds smaller rulesets than C4.5 while being more accurate.

> [CART](https://en.wikipedia.org/wiki/Predictive_analytics#Classification_and_regression_trees_.28CART.29) (Classification and Regression Trees) is very similar to C4.5, but it differs in that it supports numerical target variables (regression) and does not compute rule sets. CART constructs binary trees using the feature and threshold that yield the largest information gain at each node.

> **scikit-learn uses an optimised version of the CART algorithm; however, scikit-learn implementation does not support categorical variables for now.**

以上的决策树训练的时候，一般会采取**Cross-Validation**法：比如一共有10组数据：

第一次. 1到9做训练数据， 10做测试数据

第二次. 2到10做训练数据，1做测试数据

第三次. 1，3到10做训练数据，2做测试数据，以此类推

做10次，然后大平均错误率。这样称为 10 folds Cross-Validation。

比如 3 folds Cross-Validation 指的是数据分3份，2份做训练，1份做测试。